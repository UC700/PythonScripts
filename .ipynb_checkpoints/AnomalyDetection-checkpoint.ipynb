{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6330d9c8-b190-497a-b0a8-bbe33b61e292",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Data Connection Configuration\n",
    "db_params = {\n",
    "    'host': 'nifi1-in.unicommerce.infra',\n",
    "    'port': '5432',\n",
    "    'user': 'uniware_write',\n",
    "    'password': 'uniware@1234',\n",
    "    'database': 'dwh',\n",
    "}\n",
    "\n",
    "conn = psycopg2.connect(**db_params)\n",
    "\n",
    "# Reading data from DWH\n",
    "\n",
    "query = \"\"\" select tenant,facility,channel_code,channel_order_created_date as date,\n",
    "count(*) as shipment_count,\n",
    "sum(case when coalesce(extract(epoch from uniware_creation_timestamp - channel_order_creation_time)/60,0) + coalesce(shipment_creation_time,0) + coalesce(picklist_creation_time,0) + coalesce(picking_time,0) + coalesce(packing_time,0) > extract(epoch from fulfillment_time - channel_order_creation_time)/60 then 1 else 0 end) as breached_shipment_count\n",
    "from insights_o2sla group by 1,2,3,4 \"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "aggregate_columns_2 = ['picking_time', 'packing_time','manifest_time','dispatch_time']\n",
    "\n",
    "\n",
    "def sql_query(df,column):\n",
    "    \n",
    "    df_new = []\n",
    "    \n",
    "    median_column = \"median_\" + column\n",
    "    mean_column = \"mean_\" + column\n",
    "\n",
    "    query = f\"\"\"\n",
    "    SELECT\n",
    "        tenant,\n",
    "        facility,\n",
    "        channel_code,\n",
    "        channel_order_created_date AS date,\n",
    "        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY {column}) AS {median_column},\n",
    "        AVG({column}) AS {mean_column}\n",
    "    FROM\n",
    "        insights_o2sla\n",
    "    WHERE\n",
    "        {column} IS NOT NULL\n",
    "    GROUP BY\n",
    "        1, 2, 3, 4\n",
    "    \"\"\"\n",
    "\n",
    "    df_new = pd.read_sql_query(query, conn)\n",
    "    df = pd.merge(df, df_new, how='left', left_on=['tenant', 'facility', 'channel_code', 'date'], right_on=['tenant', 'facility', 'channel_code', 'date'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "for column in aggregate_columns_2:\n",
    "    df = sql_query(df,column)\n",
    "\n",
    "\n",
    "aggregate_columns = ['median_picking_time', 'median_packing_time','median_manifest_time','median_dispatch_time' ]\n",
    "\n",
    "# Function to calculate Z-score\n",
    "\n",
    "def modified_z_score(data):\n",
    "    median_value = np.median(data)\n",
    "    mad_value = np.median(np.abs(data - median_value))\n",
    "    \n",
    "    modified_z_scores = 0.6745 * (data - median_value) / mad_value\n",
    "    \n",
    "    return modified_z_scores\n",
    "\n",
    "    \n",
    "# Calculate Z-scores for each column within groups at facility and facility-channel level\n",
    "for column in aggregate_columns:\n",
    "    z_score_column = f'{column}_z_score'\n",
    "    df_new = []\n",
    "    df_new = df[['tenant', 'facility', 'channel_code', 'date', column]].dropna()\n",
    "    df_new[z_score_column] = df_new.groupby(['tenant','facility', 'channel_code'])[column].transform(modified_z_score)\n",
    "    df = pd.merge(df, df_new[['tenant', 'facility', 'channel_code', 'date',z_score_column]], how='left', on=['tenant', 'facility', 'channel_code', 'date'])\n",
    "\n",
    "\n",
    "start_date_last_30_days = (datetime.now() - timedelta(days=30)).date()\n",
    "\n",
    "# Filter rows where the 'date' column is within the last 30 days\n",
    "df = df[df['date'] >= start_date_last_30_days]\n",
    "\n",
    " # Calculating 25th percentile of shipment count for each tenant X facility and then filtering the rest of data\n",
    "quartile_data = df.groupby(['tenant','facility'])['shipment_count'].describe(percentiles=[.25])\n",
    "columns_to_drop = ['count','mean', 'std','min','50%','max']\n",
    "quartile_data.drop(columns=columns_to_drop, inplace=True)\n",
    "df = pd.merge(df, quartile_data, how='inner', left_on= ['tenant','facility'] , right_on= ['tenant','facility'])\n",
    "df = df[df['shipment_count'] > df['25%']]\n",
    "df.drop(columns = '25%', inplace=True)\n",
    "\n",
    "# Filtering data basis of SLA Breach Threshold of 0.5 %\n",
    "df['breach_percent'] = 100.00*df['breached_shipment_count'] / df['shipment_count']\n",
    "df = df[df['breach_percent'] > 0.5]\n",
    "\n",
    "\n",
    "engine = create_engine(\"postgresql+psycopg2://uniware_write:uniware%401234@nifi1-in.unicommerce.infra:5432/dwh\")\n",
    "\n",
    "# Specify the name of the PostgreSQL table where you want to insert the data\n",
    "table_name = 'insights_o2sla_anomaly'\n",
    "\n",
    "engine.execute(\"truncate table insights_o2sla_anomaly\")\n",
    "\n",
    "# # Insert the DataFrame into the PostgreSQL table\n",
    "df.to_sql(table_name, engine.connect(),if_exists='replace', index=False)\n",
    "\n",
    "# # Close the SQLAlchemy engine (optional)\n",
    "engine.dispose()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
